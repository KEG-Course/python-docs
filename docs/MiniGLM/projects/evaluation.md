# 模型推理与评估

## 模型推理

在本次作业中，我们需要将训练好的模型能够在本地正常运行起来。调试可以通过命令行输出观察模型效果。

- 在进行展示时，要求大家以可视化的界面（网页）进行展示与模型的对话，便于实验的测试与验收。推荐使用 [Gradio](https://www.gradio.app/guides/quickstart) 来进行实现。
- 可以将在训练完的模型从服务器上传输到本地，运行推理来进行 gradio 展示。

## 模型评估

我们将采用以下三方面的指标进行评价模型训练的结果。这些指标旨在高效地指导模型拟合数据分布，同时使人客观地评价模型生成文本的质量。

### 传统指标（自行评估）

设置合理的损失函数。在本次实验中，训练损失函数即为单个词预测的交叉熵损失（cross-entropy loss）的平均。

#### 困惑度

* 困惑度（Perlexity）是刻画语言模型预测一个语言文本的能力，表示模型预测某一个句子出现的合理程度。一般用模型预测一个句子中所有词出现的概率来计算。在测试集上，困惑度值越低，说明建模的效果越好。
* 困惑度的计算方式如下：

$$
\begin{split}
\text{Perlexity(S)} &= p(w_1, w_2, w_3, \cdots, w_m)^{-1/m}\\
&=[p(w_1)p(w_2|w_1)...p(w_m|w_1,...,w_{m-1})]^{(-1/m)}
\end{split}
$$

其中，$m$是句子的长度。

#### Rouge-L

* Rouge是通过将模型生成的回答（Y）与参考答案（X）进行比较计算，得到对应的得分。
* Rouge-L利用最长公共子序列进行计算，注意子序列不一定连续，但是有词的顺序。
* Rouge-L的计算方式如下：

$$
\text{Rouge-L(lcs)} = \frac{(1+\beta^2)R\_{lcs}P\_{lcs}}{R\_{lcs} + \beta^2P\_{lcs}}
$$

$$
R_{(lcs)} = \frac{LCS(X, Y)}{m}
$$

$$
P_{(lcs)} = \frac{LCS(X, Y)}{n}
$$

其中，$\beta$是超参数，表示句子的长度，m和n各是X和Y的长度。

### 质量评价（验收）

* 流畅度：
    * 没有乱码
        * 例如，不会生成类似看不懂的文本，“のて72[UNK]1904屹”。
      * 没有重复内容
        * 例如，不会生成类似“2021等2021等2021等2021等2021等”这样的文本。
    * 没有空白，即回答为空  
* 没有截断，即没有生成完整的一句话
    * 例如，不会生成类似“今天，我们讲 ”这样的文本。
* 文本：生成有效的最长的文本
    * 正确样例如下：
    ```
    问题是“微观经济学和宏观经济学分别研究哪些内容？”
    答案是“微观经济学检视一个社会里基本层次的行为，包括个体的行为者（例如个人、公司、买家或卖家）以及与市场的互动；宏观经济学则分析整个经济体和其议题，包括失业、通货膨胀、经济成长、财政和货币政策等。”。
    ```
* 相关度，即回答内容与问题相关
    * 例如，不会出现类似样例。即问题是“微观经济学和宏观经济学分别研究哪些内容？”，答案是“《三国演义》是四大名著之一”。
* 回复内容质量
    * 为保证公平比较，在评价时会使用助教准备好的`统一测试集`进行回复生成和质量评估。我们会用现有的语言模型（ChatGPT or ChatGLM）来比较同学生成的回复与给定标准回复的好坏，减少人工主观因素的影响。

    * **测试问题分类和样例：**

      <!-- TODO -->

### 生成速度（验收）

* 生成速度正常，评价指标（CPU / GPU 两者满足其一即可）
  * 可以在`sample.py`中设置`max_new_tokens`
  
  * max_new_tokens=256
    * GPU一般2s内输出结果。
    * 本地CPU 50s 内出结果即可（双核CPU测试结果）。
  
  * max_new_tokens=512:
    * GPU一般3.5s内输出结果。
    * 本地CPU 100s 内出结果即可（双核CPU测试结果）。